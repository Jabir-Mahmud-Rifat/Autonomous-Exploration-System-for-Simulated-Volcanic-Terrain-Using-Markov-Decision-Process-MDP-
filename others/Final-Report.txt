Abstract:
An Autonomous exploration system is crucial for navigating environments that pose high-risk factors,   such as volcanic terrains. This study explores the application of Markov Decision Process (MDP) in designing an autonomous agent capable of efficiently and safely navigating a simulated volcanic landscape. Where an agent learns to navigate safely while avoiding hazards (lava, gas, and craters) and aiming to reach the bottom-right corner of the grid. The agent operates under uncertainty, with obstacles like lava flows, craters, and gas emissions. Through the use of Q-learning, an off-policy reinforcement learning algorithm, the agent learns to balance exploration and exploitation for safe and effective navigation. In our results show that epsilon decay in the exploration strategy allows the agent to Incrementally shift from exploring new areas to exploiting known safe paths. In this performance of the agent was evaluated across various episodes, with total rewards indicating increasing efficiency and safety over time. The findings suggest that MDP-based autonomous systems can be effectively used for hazard-prone exploration tasks.
1.	Introduction
Autonomous exploration systems are an increasingly growing subject of study and very valuable because, in places where it's too dangerous or impractical for humans to go, this system becomes a necessary tool. There are many use cases such as deep ocean, outer space, war zone mine fields, and volcanic regions. These systems are made based on different intelligent decision-making models that can adapt in different changing conditions and various complex scenarios. These models also need to be smart to make choices and implement stay-safe mechanisms. One powerful approach to handle this kind of scenario is the Markov Decision Process (MDP), which helps the agent decide what action to take based on the current state and potential outcomes.

Volcanos contains very volatile areas and threats like shifting lava, toxic gases and unstable ground mean which is also constantly changing. Keeping these threats in mind our model should learn how to move and constantly adapt based on situations. It needs to explore the unknown while staying out of danger. Though past researches have shown promising results with reinforcement learning(RL) – especially Q learning can help with this. One of the toughest challenges remains the balance between exploration and sticking with the safe path at the same time. In this project and research, we will try to look at how an MDP-based approach using  Q-learning with epsilon decay can train an autonomous agent to successfully and safely navigate and explore a simulated volcanic terrain.


2. Related Works
Markov Decision Processes (MDPs) have been extensively studied in autonomous exploration, particularly for mobile robots and drones. For instance, Thrun et al. (2005) demonstrated how MDPs could be used for robot navigation, providing a framework for decision-making in uncertain environments. Kormushev et al. (2011) investigated the application of reinforcement learning in guiding robots through hazardous conditions, while Sutton and Barto (2018) offered an in-depth exploration of Q-learning within the broader scope of reinforcement learning. When it comes to volcanic terrain exploration, Yuan et al. (2020) applied machine learning techniques for detecting hazards and planning paths, underlining the necessity of real-time decision-making in such environments. Despite these advancements, the application of MDPs—particularly in managing the exploration-exploitation trade-off—has not been thoroughly explored for high-risk scenarios like volcanic terrain navigation. This research seeks to fill this gap by implementing Q-learning with epsilon decay to strike a balance between exploration and safety while navigating volcanic environments.


3. Problem Definition and Approach
3.1 Problem Overview
The goal of our project is to design an intelligent, self-operating agent that is able to safely and successfully explore a simulated volcanic environment which is full of hazards for example: lava, craters, and toxic gases. The agent first learn to make smart decisions— choose a paths, let it explore as much as possible, steering clear of danger. To achieve this goal, we use a Markov Decision Process (MDP), a framework where the agent interacts with its surroundings and learns from the rewards it gets, eventually it will figure out the best way to move through the volcanic terrain.
3.2 Understanding the MDP Framework
The Markov Decision Process we are developing have several key components:
•	States (S): These represent where the agent is on the grid and how close it is to hazards like lava or gas.
•	Actions (A): The possible moves the agent can make—up, down, left, right, or checking the area for danger.
•	Transition Function (P): This gives the likelihood of ending up in a new state after taking an action, affected by environmental factors such as lava flows or shifting gas.
•	Rewards (R): The agent gets positive points for exploring safely and penalties if it enters hazardous zones.
•	Policy (π): The agent's strategy for choosing actions based on its current state.
The main objective is to earn as many rewards as possible by exploring wisely and learning to stick to safe paths over time.

3.3 How Q-learning Works

Q-learning is a reinforcement learning method we are using to train the agent. It helps the agent learn how valuable certain actions are in different situations. After taking an action, the agent updates its estimate of that action’s quality using this formula:
Q(s,a)←Q(s,a)+α[R(s,a)+γmax⁡a′Q(s′,a′)−Q(s,a)]Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R(s, a) + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]Q(s,a)←Q(s,a)+α[R(s,a)+γa′maxQ(s′,a′)−Q(s,a)] 
Where:
•	α (alpha) is the learning rate—how quickly the agent updates what it knows.
•	γ (gamma) is the discount factor, which determines how much future rewards matter.
•	R(s, a) is the reward earned by doing action a in state s.
•	Q(s', a') is the estimated best future reward from the next state.
This process repeats as the agent keeps learning what choices lead to better outcomes.
3.4 Epsilon Decay Strategy
To make the agent achieve the balance between trying new things (exploration) and sticking with what works (exploitation), we use an epsilon decay strategy. At first, the agent explores a lot, making random choices with a high epsilon value. Over time, epsilon decreases, encouraging the agent to use the knowledge it's gained. After every episode, epsilon is updated like this:
ϵ←ϵ×EPSILON_DECAY\epsilon \leftarrow \epsilon \times \text{EPSILON\_DECAY}ϵ←ϵ×EPSILON_DECAY 
This controlled decay (limited by a minimum threshold) helps the agent become more efficient as it learns.

7. Conclusion
In this study, the successful application of the Markov Decision Process (MDP) and Q-learning with epsilon decay for autonomous exploration in a simulated volcanic terrain is demonstrated. The agent learned to navigate safely while avoiding hazards (lava, gas, and craters) and was able to reach the bottom-right corner of the grid. The highlight of the result is the potential of MDP-based models in autonomous systems designed for high-risk exploration tasks. The use of epsilon decay proved its importance in optimizing the exploration-exploitation trade-off, making it highly applicable for real-world scenarios requiring adaptive decision-making under uncertainty.



