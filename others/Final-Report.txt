Abstract:
Introduction:
1.	Introduction
Autonomous exploration systems are an increasingly growing subject of study and very valuable because, in places where it's too dangerous or impractical for humans to go, this system becomes a necessary tool. There are many use cases such as deep ocean, outer space, war zone mine fields, and volcanic regions. These systems are made based on different intelligent decision-making models that can adapt in different changing conditions and various complex scenarios. These models also need to be smart to make choices and implement stay-safe mechanisms. One powerful approach to handle this kind of scenario is the Markov Decision Process (MDP), which helps the agent decide what action to take based on the current state and potential outcomes.

Volcanos contains very volatile areas and threats like shifting lava, toxic gases and unstable ground mean which is also constantly changing. Keeping these threats in mind our model should learn how to move and constantly adapt based on situations. It needs to explore the unknown while staying out of danger. Though past researches have shown promising results with reinforcement learning(RL) – especially Q learning can help with this. One of the toughest challenges remains the balance between exploration and sticking with the safe path at the same time. In this project and research, we will try to look at how an MDP-based approach using  Q-learning with epsilon decay can train an autonomous agent to successfully and safely navigate and explore a simulated volcanic terrain.




Markov Decision Processes (MDPs) have been extensively studied in autonomous exploration, particularly for mobile robots and drones. For instance, Thrun et al. (2005) demonstrated how MDPs could be used for robot navigation, providing a framework for decision-making in uncertain environments. Kormushev et al. (2011) investigated the application of reinforcement learning in guiding robots through hazardous conditions, while Sutton and Barto (2018) offered an in-depth exploration of Q-learning within the broader scope of reinforcement learning. When it comes to volcanic terrain exploration, Yuan et al. (2020) applied machine learning techniques for detecting hazards and planning paths, underlining the necessity of real-time decision-making in such environments. Despite these advancements, the application of MDPs—particularly in managing the exploration-exploitation trade-off—has not been thoroughly explored for high-risk scenarios like volcanic terrain navigation. This research seeks to fill this gap by implementing Q-learning with epsilon decay to strike a balance between exploration and safety while navigating volcanic environments.






